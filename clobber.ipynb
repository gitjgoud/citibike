{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trips and Stations\n",
    "* Create yearly trip parquet files\n",
    "* Create bike dock stations parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import logging\n",
    "import requests, json\n",
    "import urllib\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "# CSV_DIR = DATA_DIR + \"tripdata_csv/\"\n",
    "PARQUET_DIR = DATA_DIR + \"tripdata_parquet/\"\n",
    "NY_DIR = PARQUET_DIR + \"NY/\"\n",
    "NJ_DIR = PARQUET_DIR + \"NJ/\"\n",
    "STATIONS_DIR = DATA_DIR + \"stations/\"\n",
    "PARQUET_EXTENSION = \".parquet\"\n",
    "# Station Information GBFS json url\n",
    "STATION_INFO_URL = \"https://gbfs.citibikenyc.com/gbfs/en/station_information.json\"\n",
    "# USGS Elevation Point Query Service url\n",
    "USGS_ELEVATION_POINT_SERVICE_URL = r\"https://nationalmap.gov/epqs/pqs.php?\"\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "logging.info(\n",
    "    f\"{len(os.listdir(NJ_DIR))} Jersey City files and {len(os.listdir(NY_DIR))} New York City files\"\n",
    ")\n",
    "\n",
    "# schema for parquet files in\n",
    "TRIPDATA_COLUMN_DTYPES = {\n",
    "    \"tripduration\": \"int32\",\n",
    "    \"starttime\": \"datetime64\",\n",
    "    \"stoptime\": \"datetime64\",\n",
    "    \"startstationid\": \"category\",\n",
    "    \"startstationname\": \"category\",\n",
    "    \"startstationlatitude\": \"category\",\n",
    "    \"startstationlongitude\": \"category\",\n",
    "    \"endstationid\": \"category\",\n",
    "    \"endstationname\": \"category\",\n",
    "    \"endstationlatitude\": \"category\",\n",
    "    \"endstationlongitude\": \"category\",\n",
    "    \"bikeid\": \"category\",\n",
    "    \"usertype\": \"category\",\n",
    "    \"birthyear\": \"category\",\n",
    "    \"gender\": \"category\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(NY_DIR):\n",
    "    os.makedirs(os.path.dirname(NY_DIR))\n",
    "\n",
    "if not os.path.exists(NJ_DIR):\n",
    "    os.makedirs(os.path.dirname(NJ_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_monthly_trips(year, directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a merged parquet file from parquet files in a directory\n",
    "    :param year: the year (int) to merge monthly data for. if None, then merge all files in directory\n",
    "    :param directory: a directory containing parquet files with identical schema (column names) across files\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if year:\n",
    "        range_start = str(year) + \"-01\"\n",
    "        range_end = str(year) + \"-13\"\n",
    "        month_files = sorted(\n",
    "            [\n",
    "                directory + f\n",
    "                for f in os.listdir(directory)\n",
    "                if range_start <= f <= range_end\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        month_files = sorted(\n",
    "            [\n",
    "                directory + f\n",
    "                for f in os.listdir(directory)\n",
    "                if f.endswith(PARQUET_EXTENSION)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    parquet_ddfs: list[dd.DataFrame] = []\n",
    "    for month_file in month_files:\n",
    "        if os.path.exists(month_file):\n",
    "            ddf = dd.read_parquet(month_file)\n",
    "            ddf.astype(TRIPDATA_COLUMN_DTYPES)\n",
    "            ddf[\"birthyear\"] = ddf[\"birthyear\"].astype(\n",
    "                \"str\"\n",
    "            )  # some issue with birthyear in particular\n",
    "            parquet_ddfs.append(ddf)\n",
    "\n",
    "    all_trips = dd.concat(parquet_ddfs)\n",
    "    filename = str(year) if year else \"alltrips\"\n",
    "    all_trips.to_parquet(\n",
    "        directory + filename + PARQUET_EXTENSION,\n",
    "        schema={\"birthyear\": pa.string()},\n",
    "        engine=\"pyarrow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create parquet file from all trip data (NY)\n",
    "# NOTE run this before running the below cell if you want this large file. running it after will not work\n",
    "merge_monthly_trips(year=None, directory=NY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create yearly trip data parquet files\n",
    "for year in range(2013, 2022):\n",
    "    merge_monthly_trips(year, NY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# example: read a yearly parquet file (2019)\n",
    "\n",
    "trip_columns = [\n",
    "    \"tripduration\",\n",
    "    \"starttime\",\n",
    "    \"stoptime\",\n",
    "    \"startstationid\",\n",
    "    \"endstationid\",\n",
    "    \"bikeid\",\n",
    "    \"usertype\",\n",
    "    \"birthyear\",\n",
    "    \"gender\",\n",
    "]  # specify columns you want to read\n",
    "test = pd.read_parquet(\n",
    "    NY_DIR + \"2019.parquet\", engine=\"pyarrow\", columns=trip_columns\n",
    ").reset_index()\n",
    "test.drop(test.columns[0], axis=1, inplace=True)  # drop the dask index\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(STATIONS_DIR):\n",
    "    os.makedirs(os.path.dirname(STATIONS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_stations(year, directory):\n",
    "    \"\"\"\n",
    "    Creates station table for year, saves to parquet file\n",
    "    :param year: year to create stations for using trip data for that year\n",
    "    :param directory: directory with the trip data parquet file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    trip_filepath = directory + str(year) + PARQUET_EXTENSION\n",
    "    trips = pd.read_parquet(trip_filepath, engine=\"pyarrow\").reset_index()\n",
    "    trips.drop(trips.columns[0], axis=1, inplace=True)  # drop the dask index\n",
    "\n",
    "    station_columns = [\n",
    "        \"startstationid\",\n",
    "        \"startstationname\",\n",
    "        \"startstationlatitude\",\n",
    "        \"startstationlongitude\",\n",
    "    ]\n",
    "    stations = trips[station_columns]\n",
    "    col_rename = {\n",
    "        \"startstationid\": \"stationid\",\n",
    "        \"startstationname\": \"stationname\",\n",
    "        \"startstationlatitude\": \"latitude\",\n",
    "        \"startstationlongitude\": \"longitude\",\n",
    "    }\n",
    "    stations.rename(columns=col_rename, inplace=True)\n",
    "    stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "\n",
    "    stations_filepath = STATIONS_DIR + str(year) + PARQUET_EXTENSION\n",
    "    stations.to_parquet(stations_filepath, engine=\"pyarrow\")\n",
    "\n",
    "    # when reading a trip parquet file, just specify the columns to use\n",
    "    # # remove unneeded cols from trips and save back to itself\n",
    "    # drop_cols = [\n",
    "    #     \"startstationname\",\n",
    "    #     \"startstationlatitude\",\n",
    "    #     \"startstationlongitude\",\n",
    "    #     \"endstationname\",\n",
    "    #     \"endstationlatitude\",\n",
    "    #     \"endstationlongitude\",\n",
    "    # ]\n",
    "    # trips.drop(drop_cols, axis=1, inplace=True)\n",
    "    # trips.to_parquet(directory + str(year) + PARQUET_EXTENSION, schema={\"birthyear\": pa.string()}, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for year in range(2013, 2022):\n",
    "    create_stations(year, NY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_stations() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return merged yearly station files\n",
    "    \"\"\"\n",
    "    stations_dfs = []\n",
    "    stations_files = [\n",
    "        f for f in os.listdir(STATIONS_DIR) if not f.startswith(\"stations\")\n",
    "    ]\n",
    "    for station_file in stations_files:\n",
    "        filepath = STATIONS_DIR + station_file\n",
    "        stations_dfs.append(pd.read_parquet(filepath))\n",
    "\n",
    "    all_stations = pd.concat(stations_dfs)\n",
    "    all_stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "\n",
    "    return all_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_station_capacity(stations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds station capacity info from Citibike GBFS feed\n",
    "    :param stations:\n",
    "    :return: stations with capacity info\n",
    "    \"\"\"\n",
    "    # get station info\n",
    "    url = requests.get(STATION_INFO_URL)\n",
    "    data = json.loads(url.text)\n",
    "    station_details = pd.DataFrame.from_dict(data[\"data\"][\"stations\"])\n",
    "\n",
    "    # extract capacity and merge back to dataframe\n",
    "    station_details = station_details[[\"name\", \"capacity\"]]\n",
    "    station_details.rename(columns={\"name\": \"stationname\"}, inplace=True)\n",
    "\n",
    "    return stations.merge(station_details, how=\"left\", on=\"stationname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_station_geodata(stations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds station geodata info\n",
    "    :param stations:\n",
    "    :return: stations df with geodata info\n",
    "    \"\"\"\n",
    "    logging.debug(\"reverse geocoding boro and neighbourhood, wait 15-20 mins...\")\n",
    "    geolocator = Nominatim(user_agent=\"bikegeocode\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1)\n",
    "    locations_lst = []\n",
    "    for index, row in stations.iterrows():\n",
    "        locations_lst.append(\n",
    "            reverse(\"{}, {}\".format(row[\"latitude\"], row[\"longitude\"])).raw[\"address\"]\n",
    "        )\n",
    "    logging.debug(\"geocode complete, merging...\")\n",
    "    locations = pd.DataFrame(locations_lst, index=stations.stationid).reset_index()\n",
    "    locations = locations[[\"stationid\", \"neighbourhood\", \"suburb\", \"postcode\"]]\n",
    "    locations.rename(columns={\"suburb\": \"boro\", \"postcode\": \"zipcode\"}, inplace=True)\n",
    "    locations = locations.astype(\"category\")\n",
    "\n",
    "    return stations.merge(locations, how=\"left\", on=\"stationid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_elevations(df: pd.DataFrame, lat_column=\"latitude\", lon_column=\"longitude\"):\n",
    "    \"\"\"Queries USGS Elevation Point Service to get elevation values\n",
    "\n",
    "    :param df: dataframe with latitude and longitude\n",
    "    :param lat_column:\n",
    "    :param lon_column:\n",
    "    :return: original df with new elevation column\n",
    "    \"\"\"\n",
    "    elevations = []\n",
    "    i = 0\n",
    "    for lat, lon in zip(df[lat_column], df[lon_column]):\n",
    "        i += 1\n",
    "        logging.debug(f\"Getting elevation {i} for ({lat}, {lon})\")\n",
    "        # define rest query params\n",
    "        params = {\"output\": \"json\", \"x\": lon, \"y\": lat, \"units\": \"Feet\"}\n",
    "\n",
    "        # format query string and return query value\n",
    "        result = requests.get((url + urllib.parse.urlencode(params)))\n",
    "        elevations.append(\n",
    "            result.json()[\"USGS_Elevation_Point_Query_Service\"][\"Elevation_Query\"][\n",
    "                \"Elevation\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    df[\"elevation_ft\"] = elevations\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# merge yearly stations data, get capacity, get geodata, save\n",
    "# TODO get elevation\n",
    "stations = merge_stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stations = add_station_capacity(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stations = add_station_geodata(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stations = add_elevations(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stations.to_csv(STATIONS_DIR + \"stations\" + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stations[\"elevation_ft\"] = stations[\"elevation_ft\"].astype(\"str\")\n",
    "stations.to_parquet(STATIONS_DIR + \"stations\" + PARQUET_EXTENSION, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# example: read stations (all stations seen across all years)\n",
    "stations = pd.read_parquet(\n",
    "    STATIONS_DIR + \"stations\" + PARQUET_EXTENSION, engine=\"pyarrow\"\n",
    ")\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
