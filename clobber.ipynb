{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clobber Together Source Data\n",
    "[Summarize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import gc\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "CSV_DIR = DATA_DIR + \"tripdata_csv/\"\n",
    "NY_DIR = CSV_DIR + \"NY/\"\n",
    "NJ_DIR = CSV_DIR + \"NJ/\"\n",
    "\n",
    "DB_FILE = \"data/tripdata.db\"\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "JC_DATA = os.listdir(NJ_DIR)  # NOTE: this includes Hoboken and Jersey City\n",
    "NYC_DATA = os.listdir(NY_DIR)\n",
    "\n",
    "logging.info(\n",
    "    f\"{len(JC_DATA)} Jersey City files and {len(NYC_DATA)} New York City files\"\n",
    ")\n",
    "\n",
    "SCHEMA_CHANGE_DATE = \"2021-02\"\n",
    "\n",
    "# CSV paths for NYC, JC (pre and post schema change)\n",
    "nyc_old = sorted([NY_DIR + f for f in os.listdir(NY_DIR) if f < SCHEMA_CHANGE_DATE])\n",
    "nyc_new = sorted([NY_DIR + f for f in os.listdir(NY_DIR) if f >= SCHEMA_CHANGE_DATE])\n",
    "\n",
    "jc_old = sorted([NJ_DIR + f for f in os.listdir(NJ_DIR) if f < SCHEMA_CHANGE_DATE])\n",
    "jc_new = sorted([NJ_DIR + f for f in os.listdir(NJ_DIR) if f >= SCHEMA_CHANGE_DATE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Annual Data Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_year(year=2019, state=\"NY\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe from source CSVs that is all monthly trip data for that `year`\n",
    "\n",
    "    :param year: the year for which to concatenate data files\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: the merged dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    range_start = str(year) + \"-01\"\n",
    "    range_end = str(year) + \"-13\"  # Not sure why I have to select 13 here...\n",
    "    files = None\n",
    "    if state == \"NY\":\n",
    "        files = sorted(\n",
    "            [NY_DIR + f for f in os.listdir(NY_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    elif state == \"NJ\":\n",
    "        files = sorted(\n",
    "            [NJ_DIR + f for f in os.listdir(NJ_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    else:\n",
    "        raise IndexError(f\"No data for state: {state}\")\n",
    "\n",
    "    logging.debug(f\"Will merge these files: {files}, number of files: {len(files)}\")\n",
    "\n",
    "    # Concatenate all monthly data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"startstationname\": \"category\",\n",
    "        \"startstationlatitude\": \"category\",\n",
    "        \"startstationlongitude\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"endstationname\": \"category\",\n",
    "        \"endstationlatitude\": \"category\",\n",
    "        \"endstationlongitude\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_csv(file)\n",
    "        print(\"formatting columns...\" + file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        # for some reason runs faster with dropped NA and converting temp df types\n",
    "        df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "        df = df.astype(coltypes)\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes - category conversion lost on concat\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(year, \"...dtypes converted\")\n",
    "\n",
    "    return clobbered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_year_data_files(years=[2019], state=\"NY\"):\n",
    "    \"\"\"\n",
    "    Calls clobber_year, separates ride and station data, writes output to both csv and parquet to `data/`\n",
    "\n",
    "    :param years: list of years to generate data files for\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    for year in years:\n",
    "        # clobber dataframe\n",
    "        print(\"clobbering...\", year)\n",
    "        rides = clobber_year(year, state)\n",
    "        gc.collect()\n",
    "        print(year, \"...clobbered\")\n",
    "\n",
    "        # extract station data and save to file\n",
    "        print(\"extracting stations...\", year)\n",
    "        stations = pd.DataFrame()\n",
    "        col_select = [\n",
    "            \"startstationid\",\n",
    "            \"startstationname\",\n",
    "            \"startstationlatitude\",\n",
    "            \"startstationlongitude\",\n",
    "        ]\n",
    "        stations = rides[col_select]\n",
    "        col_rename = {\n",
    "            \"startstationid\": \"stationid\",\n",
    "            \"startstationname\": \"stationname\",\n",
    "            \"startstationlatitude\": \"latitude\",\n",
    "            \"startstationlongitude\": \"longitude\",\n",
    "        }\n",
    "        stations.rename(columns=col_rename, inplace=True)\n",
    "        stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "        exportpath = \"data/stations_\" + str(year) + \".parquet\"\n",
    "        stations.to_parquet(exportpath)\n",
    "        del stations\n",
    "        gc.collect()\n",
    "        print(year, \"...stations extracted & saved\")\n",
    "\n",
    "        # remove uneeded cols from rides\n",
    "        drop_cols = [\n",
    "            \"startstationname\",\n",
    "            \"startstationlatitude\",\n",
    "            \"startstationlongitude\",\n",
    "            \"endstationname\",\n",
    "            \"endstationlatitude\",\n",
    "            \"endstationlongitude\",\n",
    "        ]\n",
    "        rides.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "        # save ride to file\n",
    "        exportpath = \"data/rides_\" + str(year) + \".parquet\"\n",
    "        rides.to_parquet(exportpath)\n",
    "        print(year, \"...saved to parquet\")\n",
    "\n",
    "        # exportpath = \"data/rides_\" + str(year) + '.csv'\n",
    "        # rides.to_csv(exportpath)\n",
    "        # print(year,'...saved to csv')\n",
    "\n",
    "        # unload rides dataframe\n",
    "        del rides\n",
    "        gc.collect()\n",
    "        print(year, \"...unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clobbering... 2019\n",
      "loading...data/tripdata_csv/NY/2019-01.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-01.csv\n",
      "concating...data/tripdata_csv/NY/2019-01.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-01.csv\n",
      "loading...data/tripdata_csv/NY/2019-02.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-02.csv\n",
      "concating...data/tripdata_csv/NY/2019-02.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-02.csv\n",
      "loading...data/tripdata_csv/NY/2019-03.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-03.csv\n",
      "concating...data/tripdata_csv/NY/2019-03.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-03.csv\n",
      "loading...data/tripdata_csv/NY/2019-04.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-04.csv\n",
      "concating...data/tripdata_csv/NY/2019-04.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-04.csv\n",
      "loading...data/tripdata_csv/NY/2019-05.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-05.csv\n",
      "concating...data/tripdata_csv/NY/2019-05.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-05.csv\n",
      "loading...data/tripdata_csv/NY/2019-06.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-06.csv\n",
      "concating...data/tripdata_csv/NY/2019-06.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-06.csv\n",
      "loading...data/tripdata_csv/NY/2019-07.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-07.csv\n",
      "concating...data/tripdata_csv/NY/2019-07.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-07.csv\n",
      "loading...data/tripdata_csv/NY/2019-08.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-08.csv\n",
      "concating...data/tripdata_csv/NY/2019-08.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-08.csv\n",
      "loading...data/tripdata_csv/NY/2019-09.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-09.csv\n",
      "concating...data/tripdata_csv/NY/2019-09.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-09.csv\n",
      "loading...data/tripdata_csv/NY/2019-10.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-10.csv\n",
      "concating...data/tripdata_csv/NY/2019-10.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-10.csv\n",
      "loading...data/tripdata_csv/NY/2019-11.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-11.csv\n",
      "concating...data/tripdata_csv/NY/2019-11.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-11.csv\n",
      "loading...data/tripdata_csv/NY/2019-12.csv\n",
      "formatting columns...data/tripdata_csv/NY/2019-12.csv\n",
      "concating...data/tripdata_csv/NY/2019-12.csv\n",
      "unloaded...data/tripdata_csv/NY/2019-12.csv\n",
      "2019 ...dtypes converted\n",
      "2019 ...clobbered\n",
      "extracting stations... 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_379/1267125294.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations.rename(columns=col_rename, inplace=True)\n",
      "/tmp/ipykernel_379/1267125294.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 936 entries, 0 to 20428133\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   stationid    936 non-null    category\n",
      " 1   stationname  936 non-null    category\n",
      " 2   latitude     936 non-null    category\n",
      " 3   longitude    936 non-null    category\n",
      "dtypes: category(4)\n",
      "memory usage: 173.5 KB\n",
      "None\n",
      "stationid      0\n",
      "stationname    0\n",
      "latitude       0\n",
      "longitude      0\n",
      "dtype: int64\n",
      "2019 ...stations extracted & saved\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20551517 entries, 0 to 20551516\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   tripduration    int32         \n",
      " 1   starttime       datetime64[ns]\n",
      " 2   stoptime        datetime64[ns]\n",
      " 3   startstationid  category      \n",
      " 4   endstationid    category      \n",
      " 5   bikeid          category      \n",
      " 6   usertype        category      \n",
      " 7   birthyear       category      \n",
      " 8   gender          category      \n",
      "dtypes: category(6), datetime64[ns](2), int32(1)\n",
      "memory usage: 569.1 MB\n",
      "None\n",
      "tripduration      0\n",
      "starttime         0\n",
      "stoptime          0\n",
      "startstationid    0\n",
      "endstationid      0\n",
      "bikeid            0\n",
      "usertype          0\n",
      "birthyear         0\n",
      "gender            0\n",
      "dtype: int64\n",
      "2019 ...saved to parquet\n",
      "2019 ...unloaded\n",
      "clobbering... 2018\n",
      "loading...data/tripdata_csv/NY/2018-01.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-01.csv\n",
      "concating...data/tripdata_csv/NY/2018-01.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-01.csv\n",
      "loading...data/tripdata_csv/NY/2018-02.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-02.csv\n",
      "concating...data/tripdata_csv/NY/2018-02.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-02.csv\n",
      "loading...data/tripdata_csv/NY/2018-03.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-03.csv\n",
      "concating...data/tripdata_csv/NY/2018-03.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-03.csv\n",
      "loading...data/tripdata_csv/NY/2018-04.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-04.csv\n",
      "concating...data/tripdata_csv/NY/2018-04.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-04.csv\n",
      "loading...data/tripdata_csv/NY/2018-05.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-05.csv\n",
      "concating...data/tripdata_csv/NY/2018-05.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-05.csv\n",
      "loading...data/tripdata_csv/NY/2018-06.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-06.csv\n",
      "concating...data/tripdata_csv/NY/2018-06.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-06.csv\n",
      "loading...data/tripdata_csv/NY/2018-07.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-07.csv\n",
      "concating...data/tripdata_csv/NY/2018-07.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-07.csv\n",
      "loading...data/tripdata_csv/NY/2018-08.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-08.csv\n",
      "concating...data/tripdata_csv/NY/2018-08.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-08.csv\n",
      "loading...data/tripdata_csv/NY/2018-09.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-09.csv\n",
      "concating...data/tripdata_csv/NY/2018-09.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-09.csv\n",
      "loading...data/tripdata_csv/NY/2018-10.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-10.csv\n",
      "concating...data/tripdata_csv/NY/2018-10.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-10.csv\n",
      "loading...data/tripdata_csv/NY/2018-11.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-11.csv\n",
      "concating...data/tripdata_csv/NY/2018-11.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-11.csv\n",
      "loading...data/tripdata_csv/NY/2018-12.csv\n",
      "formatting columns...data/tripdata_csv/NY/2018-12.csv\n",
      "concating...data/tripdata_csv/NY/2018-12.csv\n",
      "unloaded...data/tripdata_csv/NY/2018-12.csv\n",
      "2018 ...dtypes converted\n",
      "2018 ...clobbered\n",
      "extracting stations... 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_379/1267125294.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations.rename(columns=col_rename, inplace=True)\n",
      "/tmp/ipykernel_379/1267125294.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 818 entries, 0 to 17065613\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   stationid    818 non-null    category\n",
      " 1   stationname  818 non-null    category\n",
      " 2   latitude     818 non-null    category\n",
      " 3   longitude    818 non-null    category\n",
      "dtypes: category(4)\n",
      "memory usage: 167.5 KB\n",
      "None\n",
      "stationid      0\n",
      "stationname    0\n",
      "latitude       0\n",
      "longitude      0\n",
      "dtype: int64\n",
      "2018 ...stations extracted & saved\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17545842 entries, 0 to 17545841\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   tripduration    int32         \n",
      " 1   starttime       datetime64[ns]\n",
      " 2   stoptime        datetime64[ns]\n",
      " 3   startstationid  category      \n",
      " 4   endstationid    category      \n",
      " 5   bikeid          category      \n",
      " 6   usertype        category      \n",
      " 7   birthyear       category      \n",
      " 8   gender          category      \n",
      "dtypes: category(6), datetime64[ns](2), int32(1)\n",
      "memory usage: 486.0 MB\n",
      "None\n",
      "tripduration      0\n",
      "starttime         0\n",
      "stoptime          0\n",
      "startstationid    0\n",
      "endstationid      0\n",
      "bikeid            0\n",
      "usertype          0\n",
      "birthyear         0\n",
      "gender            0\n",
      "dtype: int64\n",
      "2018 ...saved to parquet\n",
      "2018 ...unloaded\n",
      "clobbering... 2017\n",
      "loading...data/tripdata_csv/NY/2017-01.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-01.csv\n",
      "concating...data/tripdata_csv/NY/2017-01.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-01.csv\n",
      "loading...data/tripdata_csv/NY/2017-02.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-02.csv\n",
      "concating...data/tripdata_csv/NY/2017-02.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-02.csv\n",
      "loading...data/tripdata_csv/NY/2017-03.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-03.csv\n",
      "concating...data/tripdata_csv/NY/2017-03.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-03.csv\n",
      "loading...data/tripdata_csv/NY/2017-04.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-04.csv\n",
      "concating...data/tripdata_csv/NY/2017-04.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-04.csv\n",
      "loading...data/tripdata_csv/NY/2017-05.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-05.csv\n",
      "concating...data/tripdata_csv/NY/2017-05.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-05.csv\n",
      "loading...data/tripdata_csv/NY/2017-06.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-06.csv\n",
      "concating...data/tripdata_csv/NY/2017-06.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-06.csv\n",
      "loading...data/tripdata_csv/NY/2017-07.csv\n",
      "formatting columns...data/tripdata_csv/NY/2017-07.csv\n",
      "concating...data/tripdata_csv/NY/2017-07.csv\n",
      "unloaded...data/tripdata_csv/NY/2017-07.csv\n",
      "loading...data/tripdata_csv/NY/2017-08.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_379/2623886606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myears\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#did not select 2013 due to concern with months not existing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgen_year_data_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# error when converting 2014 datatypes (did not save files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_379/1267125294.py\u001b[0m in \u001b[0;36mgen_year_data_files\u001b[0;34m(years, state)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# clobber dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clobbering...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclobber_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...clobbered\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_379/2928576718.py\u001b[0m in \u001b[0;36mclobber_year\u001b[0;34m(year, state)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading...\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"formatting columns...\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m_concatenate_chunks\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwarning_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = \"NY\"\n",
    "years = [\n",
    "    2019,\n",
    "    2018,\n",
    "    2017,\n",
    "    2016,\n",
    "    2015,\n",
    "    2014,\n",
    "]  # did not select 2013 due to concern with months not existing\n",
    "\n",
    "gen_year_data_files(years, state)\n",
    "\n",
    "# error when converting 2014 datatypes (did not save files)\n",
    "# ArrowInvalid: ('Could not convert 1899 with type str: tried to convert to double', 'Conversion failed for column birthyear with type category')\n",
    "# have not investigated further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Rides Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_master_rides(years=[2019]):\n",
    "\n",
    "    files = [\"data/rides_\" + str(y) + \".parquet\" for y in years]\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "\n",
    "    # concatenate all years data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_parquet(file)\n",
    "        print(\"formatting columns...\" + file)\n",
    "        df = df.astype(coltypes)\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes - category conversion lost on concat\n",
    "    print(\"converting master dtypes\")\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(\"master dtypes converted\")\n",
    "\n",
    "    # export to file\n",
    "    print(\"exporting master rides\")\n",
    "    exportpath = \"data/rides_master.parquet\"\n",
    "    clobbered.to_parquet(exportpath)\n",
    "    print(\"master rides saved to parquet\")\n",
    "\n",
    "    # clean up\n",
    "    del clobbered\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...data/rides_2019.parquet\n",
      "formatting columns...data/rides_2019.parquet\n",
      "concating...data/rides_2019.parquet\n",
      "unloaded...data/rides_2019.parquet\n",
      "loading...data/rides_2018.parquet\n",
      "formatting columns...data/rides_2018.parquet\n",
      "concating...data/rides_2018.parquet\n",
      "unloaded...data/rides_2018.parquet\n",
      "loading...data/rides_2017.parquet\n",
      "formatting columns...data/rides_2017.parquet\n",
      "concating...data/rides_2017.parquet\n",
      "unloaded...data/rides_2017.parquet\n",
      "loading...data/rides_2016.parquet\n",
      "formatting columns...data/rides_2016.parquet\n",
      "concating...data/rides_2016.parquet\n",
      "unloaded...data/rides_2016.parquet\n",
      "loading...data/rides_2015.parquet\n",
      "formatting columns...data/rides_2015.parquet\n",
      "concating...data/rides_2015.parquet\n",
      "unloaded...data/rides_2015.parquet\n",
      "converting master dtypes\n",
      "master dtypes converted\n",
      "exporting master rides\n",
      "master rides saved to parquet\n"
     ]
    }
   ],
   "source": [
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "clobber_master_rides(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Stations Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_master_stations(years=[2019]):\n",
    "\n",
    "    files = [\"data/stations_\" + str(y) + \".parquet\" for y in years]\n",
    "\n",
    "    # concatenate all years data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_parquet(file)\n",
    "        print(\"formatting...\" + file)\n",
    "        df.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "        df = df.astype(\"category\")\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes and drop duplicates\n",
    "    print(\"formatting master file\")\n",
    "    clobbered.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "    clobbered = clobbered.astype(\"category\")\n",
    "    print(\"formatting complete\")\n",
    "\n",
    "    # load external station details\n",
    "    print(\"loading external station details\")\n",
    "    url = requests.get(\"https://gbfs.citibikenyc.com/gbfs/en/station_information.json\")\n",
    "    text = url.text\n",
    "    data = json.loads(text)\n",
    "    station_details = pd.DataFrame.from_dict(data[\"data\"][\"stations\"])\n",
    "\n",
    "    # extract capacity and merge back to dataframe\n",
    "    print(\"extracting capacity\")\n",
    "    station_details = station_details[[\"name\", \"capacity\"]]\n",
    "    station_details.rename(columns={\"name\": \"stationname\"}, inplace=True)\n",
    "    station_details = station_details.astype(\"category\")\n",
    "    clobbered = clobbered.merge(station_details, how=\"left\", on=\"stationname\")\n",
    "\n",
    "    # pull geolocation data for each station\n",
    "    print(\"reverse geocoding boro and neighbourhood, wait 15-20 mins...\")\n",
    "    geolocator = Nominatim(user_agent=\"bikegeocode\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1, max_retries=0)\n",
    "    locations_lst = []\n",
    "    for index, row in clobbered.iterrows():\n",
    "        locations_lst.append(\n",
    "            reverse(\"{}, {}\".format(row[\"latitude\"], row[\"longitude\"])).raw[\"address\"]\n",
    "        )\n",
    "    print(\"geocode complete, merging...\")\n",
    "    locations = pd.DataFrame(locations_lst, index=clobbered.stationid).reset_index()\n",
    "    locations = locations[[\"stationid\", \"neighbourhood\", \"suburb\", \"postcode\"]]\n",
    "    locations.rename(columns={\"suburb\": \"boro\", \"postcode\": \"zipcode\"})\n",
    "    locations = locations.astype(\"category\")\n",
    "    clobbered = clobbered.merge(locations, how=\"left\", on=\"stationid\")\n",
    "\n",
    "    # export to file\n",
    "    print(\"exporting master stations\")\n",
    "    exportpath = \"data/stations_master.parquet\"\n",
    "    clobbered.to_parquet(exportpath)\n",
    "    print(\"master stations saved to parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...data/stations_2019.parquet\n",
      "formatting...data/stations_2019.parquet\n",
      "concating...data/stations_2019.parquet\n",
      "unloaded...data/stations_2019.parquet\n",
      "loading...data/stations_2018.parquet\n",
      "formatting...data/stations_2018.parquet\n",
      "concating...data/stations_2018.parquet\n",
      "unloaded...data/stations_2018.parquet\n",
      "formatting master file\n",
      "formatting complete\n",
      "loading external station details\n",
      "extracting capacity\n",
      "reverse geocoding boro and neighbourhood, wait 15-20 mins...\n",
      "geocode complete, merging...\n",
      "exporting master stations\n",
      "master stations saved to parquet\n"
     ]
    }
   ],
   "source": [
    "years = [2019, 2018]  # , 2017, 2016, 2015]\n",
    "clobber_master_stations(years)\n",
    "\n",
    "# there are some missing capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_parquet(\"data/stations_master.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationid</th>\n",
       "      <th>stationname</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>capacity</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3160.0</td>\n",
       "      <td>Central Park West &amp; W 76 St</td>\n",
       "      <td>40.778968</td>\n",
       "      <td>-73.973747</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10023-5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519.0</td>\n",
       "      <td>Pershing Square North</td>\n",
       "      <td>40.751873</td>\n",
       "      <td>-73.977706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murray Hill</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3171.0</td>\n",
       "      <td>Amsterdam Ave &amp; W 82 St</td>\n",
       "      <td>40.785247</td>\n",
       "      <td>-73.976673</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Manhattan Community Board 7</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504.0</td>\n",
       "      <td>1 Ave &amp; E 16 St</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Manhattan Community Board 6</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229.0</td>\n",
       "      <td>Great Jones St</td>\n",
       "      <td>40.727434</td>\n",
       "      <td>-73.993790</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NoHo</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stationid                  stationname   latitude  longitude  capacity  \\\n",
       "0     3160.0  Central Park West & W 76 St  40.778968 -73.973747      39.0   \n",
       "1      519.0        Pershing Square North  40.751873 -73.977706       NaN   \n",
       "2     3171.0      Amsterdam Ave & W 82 St  40.785247 -73.976673      39.0   \n",
       "3      504.0              1 Ave & E 16 St  40.732219 -73.981656      54.0   \n",
       "4      229.0               Great Jones St  40.727434 -73.993790      23.0   \n",
       "\n",
       "                 neighbourhood     suburb    postcode  \n",
       "0                          NaN  Manhattan  10023-5104  \n",
       "1                  Murray Hill  Manhattan       10037  \n",
       "2  Manhattan Community Board 7  Manhattan       10024  \n",
       "3  Manhattan Community Board 6  Manhattan       10009  \n",
       "4                         NoHo  Manhattan       10012  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bad code  #intentially break if run all cells is performed\n",
    "\n",
    "# clobber all old nyc CSVs NOTE THIS CRASHES COMPUTER\n",
    "\n",
    "\n",
    "# nyc_old_dfs = []\n",
    "# for file in nyc_old:\n",
    "#     print(f'file {NY_DIR + file}')\n",
    "#     df = pd.read_csv(NY_DIR + file)\n",
    "#     nyc_old_dfs.append(df)\n",
    "#\n",
    "# nyc_old_df = pd.concat(nyc_old_dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# ddf = dd.read_csv(nyc_old,\n",
    "#                   dtype={'birth year': 'object',\n",
    "#                          'end station id': 'float64'})\n",
    "#\n",
    "# # columns are Sentence Cased for some CSVs and lower cased for others\n",
    "# ddf = ddf.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ddf.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO only works for old schema at the moment\n",
    "def summarise_months(outfilename: str, months: list):\n",
    "    \"\"\"\n",
    "    Writes monthly summary given list of monthly trip data\n",
    "\n",
    "    :param outfilename: where to write the summary csv\n",
    "    :param months: list of CSVs for the monthly trip data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for file in months:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        # logging.debug(f'{file}: {list(df.columns)}')\n",
    "\n",
    "        year_month = file.split(\"/\")[-1].removesuffix(\".csv\")  # YYYYMM\n",
    "\n",
    "        summary = pd.Series(dtype=object)\n",
    "        summary[\"datetime\"] = year_month\n",
    "        summary[\"counttrips\"] = df.shape[0]\n",
    "        summary[\"meanduration\"] = df.tripduration.mean()\n",
    "        summary[\"modestartstationid\"] = df.startstationid.mode()\n",
    "        summary[\"modestartstationname\"] = df.startstationname.mode()\n",
    "        summary[\"modestartstationlatitude\"] = df.startstationlatitude.mode()\n",
    "        summary[\"modestartstationlongitude\"] = df.startstationlongitude.mode()\n",
    "        summary[\"modeendstationid\"] = df.endstationid.mode()\n",
    "        summary[\"modeendstationname\"] = df.endstationname.mode()\n",
    "        summary[\"modeendstationlatitude\"] = df.endstationlatitude.mode()\n",
    "        summary[\"modeendstationlongitude\"] = df.endstationlongitude.mode()\n",
    "\n",
    "        if \"usertype\" in df.columns:\n",
    "            summary[\"usertypevalues\"] = df.usertype.value_counts()\n",
    "        elif \"member_casual\" in df.columns:\n",
    "            summary[\"usertypevalues\"] = df.member_casual.value_counts()\n",
    "\n",
    "        if \"gender\" in df.columns:\n",
    "            summary[\"gendervalues\"] = df.gender.value_counts()\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    summary_df = pd.DataFrame()\n",
    "    summary_df = summary_df.append(\n",
    "        summaries\n",
    "    )  # TODO use concat instead to suppress warning\n",
    "    summary_df.set_index(\"datetime\")\n",
    "    summary_df.to_csv(outfilename)\n",
    "\n",
    "\n",
    "# write summary data month by month for NYC and NJ\n",
    "summarise_months(DATA_DIR + \"summary_nyc_old_schema.csv\", nyc_old)\n",
    "summarise_months(DATA_DIR + \"summary_jc_old_schema.csv\", jc_old)\n",
    "\n",
    "# read summary\n",
    "nyc_old_schema_summary = pd.read_csv(\"data/summary_nyc_old_schema.csv\", index_col=0)\n",
    "nyc_old_schema_summary\n",
    "\n",
    "# read JC summary\n",
    "jc_old_schema_summary = pd.read_csv(\"data/summary_nyc_old_schema.csv\", index_col=0)\n",
    "jc_old_schema_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original-ish Clober with logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_year(year=2019, state=\"NY\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe from source CSVs that is all monthly trip data for that `year`\n",
    "\n",
    "    :param year: the year for which to concatenate data files\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: the merged dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    range_start = str(year) + \"-01\"\n",
    "    range_end = str(year) + \"-13\"  # Not sure why I have to select 13 here...\n",
    "    files = None\n",
    "    if state == \"NY\":\n",
    "        files = sorted(\n",
    "            [NY_DIR + f for f in os.listdir(NY_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    elif state == \"NJ\":\n",
    "        files = sorted(\n",
    "            [NJ_DIR + f for f in os.listdir(NJ_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    else:\n",
    "        raise IndexError(f\"No data for state: {state}\")\n",
    "\n",
    "    logging.debug(f\"Will merge these files: {files}, number of files: {len(files)}\")\n",
    "\n",
    "    # Concatenate all monthly data in range\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        logging.debug(f\"Appending df file: {file}...\")\n",
    "        dfs.append(df)\n",
    "        del df\n",
    "        gc.collect()\n",
    "    logging.debug(f\"Merging dataframes...\")\n",
    "    clobbered = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # unload temp variables\n",
    "    del dfs\n",
    "    gc.collect()\n",
    "\n",
    "    # update dtypes (doesn't carry through concat if done on read_csv...?)\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"startstationname\": \"category\",\n",
    "        \"startstationlatitude\": \"category\",\n",
    "        \"startstationlongitude\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"endstationname\": \"category\",\n",
    "        \"endstationlatitude\": \"category\",\n",
    "        \"endstationlongitude\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(year, \"...dtypes converted\")\n",
    "\n",
    "    return clobbered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_files(\n",
    "    years=[2019], state=\"NY\"\n",
    ") -> pd.DataFrame:  # what does -> pd.dataframe do?\n",
    "    \"\"\"\n",
    "    Calls clobber_year and writes output to both csv and parquet to `data/`\n",
    "\n",
    "    :param years: list of years to generate data files for\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "    for year in years:\n",
    "        # clobber dataframe\n",
    "        print(\"clobbering...\", year)\n",
    "        temp_df = clobber_year(year, state)\n",
    "        print(year, \"...clobbered\")\n",
    "\n",
    "        # extract station data [only if uberparquet faisl]\n",
    "        # perform reverse geocode ---> later\n",
    "        # create yearly stations dataframe\n",
    "        # save to file\n",
    "        # unload related dfs\n",
    "\n",
    "        # clean dataframe\n",
    "        # drop NAs? - not implemented atm\n",
    "        # drop station cols\n",
    "\n",
    "        # save to files\n",
    "        exportpath = \"data/rides_\" + str(year) + \".parquet\"\n",
    "        temp_df.to_parquet(exportpath)\n",
    "        print(year, \"...saved to parquet\")\n",
    "\n",
    "        exportpath = \"data/rides_\" + str(year) + \".csv\"\n",
    "        temp_df.to_csv(exportpath)\n",
    "        print(year, \"...saved to csv\")\n",
    "\n",
    "        # unload dataframe\n",
    "        del temp_df\n",
    "        gc.collect()\n",
    "        print(year, \"...unloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
