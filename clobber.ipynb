{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clobber Together Source Data\n",
    "[Summarize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import gc\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "CSV_DIR = DATA_DIR + \"tripdata_csv/\"\n",
    "NY_DIR = CSV_DIR + \"NY/\"\n",
    "NJ_DIR = CSV_DIR + \"NJ/\"\n",
    "\n",
    "DB_FILE = \"data/tripdata.db\"\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "JC_DATA = os.listdir(NJ_DIR)  # NOTE: this includes Hoboken and Jersey City\n",
    "NYC_DATA = os.listdir(NY_DIR)\n",
    "\n",
    "logging.info(\n",
    "    f\"{len(JC_DATA)} Jersey City files and {len(NYC_DATA)} New York City files\"\n",
    ")\n",
    "\n",
    "SCHEMA_CHANGE_DATE = \"2021-02\"\n",
    "\n",
    "# CSV paths for NYC, JC (pre and post schema change)\n",
    "nyc_old = sorted([NY_DIR + f for f in os.listdir(NY_DIR) if f < SCHEMA_CHANGE_DATE])\n",
    "nyc_new = sorted([NY_DIR + f for f in os.listdir(NY_DIR) if f >= SCHEMA_CHANGE_DATE])\n",
    "\n",
    "jc_old = sorted([NJ_DIR + f for f in os.listdir(NJ_DIR) if f < SCHEMA_CHANGE_DATE])\n",
    "jc_new = sorted([NJ_DIR + f for f in os.listdir(NJ_DIR) if f >= SCHEMA_CHANGE_DATE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Annual Data Tables (rides and stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_year(year=2019, state=\"NY\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe from source CSVs that is all monthly trip data for that `year`\n",
    "\n",
    "    :param year: the year for which to concatenate data files\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: the merged dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    range_start = str(year) + \"-01\"\n",
    "    range_end = str(year) + \"-13\"  # Not sure why I have to select 13 here...\n",
    "    files = None\n",
    "    if state == \"NY\":\n",
    "        files = sorted(\n",
    "            [NY_DIR + f for f in os.listdir(NY_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    elif state == \"NJ\":\n",
    "        files = sorted(\n",
    "            [NJ_DIR + f for f in os.listdir(NJ_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    else:\n",
    "        raise IndexError(f\"No data for state: {state}\")\n",
    "\n",
    "    logging.debug(f\"Will merge these files: {files}, number of files: {len(files)}\")\n",
    "\n",
    "    # Concatenate all monthly data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"startstationname\": \"category\",\n",
    "        \"startstationlatitude\": \"category\",\n",
    "        \"startstationlongitude\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"endstationname\": \"category\",\n",
    "        \"endstationlatitude\": \"category\",\n",
    "        \"endstationlongitude\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_csv(file)\n",
    "        print(\"formatting columns...\" + file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        # for some reason runs faster with dropped NA and converting temp df types\n",
    "        df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "        df = df.astype(coltypes)\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes - category conversion lost on concat\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(year, \"...dtypes converted\")\n",
    "\n",
    "    return clobbered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_year_data_files(years=[2019], state=\"NY\"):\n",
    "    \"\"\"\n",
    "    Calls clobber_year, separates ride and station data, writes output to both csv and parquet to `data/`\n",
    "\n",
    "    :param years: list of years to generate data files for\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    for year in years:\n",
    "        # clobber dataframe\n",
    "        print(\"clobbering...\", year)\n",
    "        rides = clobber_year(year, state)\n",
    "        gc.collect()\n",
    "        print(year, \"...clobbered\")\n",
    "\n",
    "        # extract station data and save to file\n",
    "        print(\"extracting stations...\", year)\n",
    "        stations = pd.DataFrame()\n",
    "        col_select = [\n",
    "            \"startstationid\",\n",
    "            \"startstationname\",\n",
    "            \"startstationlatitude\",\n",
    "            \"startstationlongitude\",\n",
    "        ]\n",
    "        stations = rides[col_select]\n",
    "        col_rename = {\n",
    "            \"startstationid\": \"stationid\",\n",
    "            \"startstationname\": \"stationname\",\n",
    "            \"startstationlatitude\": \"latitude\",\n",
    "            \"startstationlongitude\": \"longitude\",\n",
    "        }\n",
    "        stations.rename(columns=col_rename, inplace=True)\n",
    "        stations.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "        exportpath = \"data/stations_\" + str(year) + \".parquet\"\n",
    "        stations.to_parquet(exportpath)\n",
    "        del stations\n",
    "        gc.collect()\n",
    "        print(year, \"...stations extracted & saved\")\n",
    "\n",
    "        # remove uneeded cols from rides\n",
    "        drop_cols = [\n",
    "            \"startstationname\",\n",
    "            \"startstationlatitude\",\n",
    "            \"startstationlongitude\",\n",
    "            \"endstationname\",\n",
    "            \"endstationlatitude\",\n",
    "            \"endstationlongitude\",\n",
    "        ]\n",
    "        rides.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "        # save ride to file\n",
    "        exportpath = \"data/rides_\" + str(year) + \".parquet\"\n",
    "        rides.to_parquet(exportpath)\n",
    "        print(year, \"...saved to parquet\")\n",
    "\n",
    "        # exportpath = \"data/rides_\" + str(year) + '.csv'\n",
    "        # rides.to_csv(exportpath)\n",
    "        # print(year,'...saved to csv')\n",
    "\n",
    "        # unload rides dataframe\n",
    "        del rides\n",
    "        gc.collect()\n",
    "        print(year, \"...unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"NY\"\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "\n",
    "gen_year_data_files(years, state)\n",
    "\n",
    "# error when converting 2014 datatypes (did not save files)\n",
    "# ArrowInvalid: ('Could not convert 1899 with type str: tried to convert to double', 'Conversion failed for column birthyear with type category')\n",
    "# have not investigated further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Rides Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_master_rides(years=[2019]):\n",
    "\n",
    "    files = [\"data/rides_\" + str(y) + \".parquet\" for y in years]\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "\n",
    "    # concatenate all years data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_parquet(file)\n",
    "        print(\"formatting columns...\" + file)\n",
    "        df = df.astype(coltypes)\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes - category conversion lost on concat\n",
    "    print(\"converting master dtypes\")\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(\"master dtypes converted\")\n",
    "\n",
    "    # export to file\n",
    "    print(\"exporting master rides\")\n",
    "    exportpath = \"data/rides_master.parquet\"\n",
    "    clobbered.to_parquet(exportpath)\n",
    "    print(\"master rides saved to parquet\")\n",
    "\n",
    "    # clean up\n",
    "    del clobbered\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...data/rides_2019.parquet\n",
      "formatting columns...data/rides_2019.parquet\n",
      "concating...data/rides_2019.parquet\n",
      "unloaded...data/rides_2019.parquet\n",
      "loading...data/rides_2018.parquet\n",
      "formatting columns...data/rides_2018.parquet\n",
      "concating...data/rides_2018.parquet\n",
      "unloaded...data/rides_2018.parquet\n",
      "loading...data/rides_2017.parquet\n",
      "formatting columns...data/rides_2017.parquet\n",
      "concating...data/rides_2017.parquet\n",
      "unloaded...data/rides_2017.parquet\n",
      "loading...data/rides_2016.parquet\n",
      "formatting columns...data/rides_2016.parquet\n",
      "concating...data/rides_2016.parquet\n",
      "unloaded...data/rides_2016.parquet\n",
      "loading...data/rides_2015.parquet\n",
      "formatting columns...data/rides_2015.parquet\n",
      "concating...data/rides_2015.parquet\n",
      "unloaded...data/rides_2015.parquet\n",
      "converting master dtypes\n",
      "master dtypes converted\n",
      "exporting master rides\n",
      "master rides saved to parquet\n"
     ]
    }
   ],
   "source": [
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "clobber_master_rides(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Stations Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_master_stations(years=[2019]):\n",
    "\n",
    "    files = [\"data/stations_\" + str(y) + \".parquet\" for y in years]\n",
    "\n",
    "    # concatenate all years data in range\n",
    "    clobbered = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(\"loading...\" + file)\n",
    "        df = pd.read_parquet(file)\n",
    "        print(\"formatting...\" + file)\n",
    "        df.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "        df = df.astype(\"category\")\n",
    "        print(\"concating...\" + file)\n",
    "        clobbered = pd.concat([clobbered, df], axis=0, ignore_index=True)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"unloaded...\" + file)\n",
    "\n",
    "    # update dtypes and drop duplicates\n",
    "    print(\"formatting master file\")\n",
    "    clobbered.drop_duplicates(subset=[\"stationid\"], inplace=True)\n",
    "    clobbered = clobbered.astype(\"category\")\n",
    "    print(\"formatting complete\")\n",
    "\n",
    "    # load external station details\n",
    "    print(\"loading external station details\")\n",
    "    url = requests.get(\"https://gbfs.citibikenyc.com/gbfs/en/station_information.json\")\n",
    "    text = url.text\n",
    "    data = json.loads(text)\n",
    "    station_details = pd.DataFrame.from_dict(data[\"data\"][\"stations\"])\n",
    "\n",
    "    # extract capacity and merge back to dataframe\n",
    "    print(\"extracting capacity\")\n",
    "    station_details = station_details[[\"name\", \"capacity\"]]\n",
    "    station_details.rename(columns={\"name\": \"stationname\"}, inplace=True)\n",
    "    station_details = station_details.astype(\"category\")\n",
    "    clobbered = clobbered.merge(station_details, how=\"left\", on=\"stationname\")\n",
    "\n",
    "    # pull geolocation data for each station\n",
    "    print(\"reverse geocoding boro and neighbourhood, wait 15-20 mins...\")\n",
    "    geolocator = Nominatim(user_agent=\"bikegeocode\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1, max_retries=0)\n",
    "    locations_lst = []\n",
    "    for index, row in clobbered.iterrows():\n",
    "        locations_lst.append(\n",
    "            reverse(\"{}, {}\".format(row[\"latitude\"], row[\"longitude\"])).raw[\"address\"]\n",
    "        )\n",
    "    print(\"geocode complete, merging...\")\n",
    "    locations = pd.DataFrame(locations_lst, index=clobbered.stationid).reset_index()\n",
    "    locations = locations[[\"stationid\", \"neighbourhood\", \"suburb\", \"postcode\"]]\n",
    "    locations.rename(columns={\"suburb\": \"boro\", \"postcode\": \"zipcode\"}, inplace=True)\n",
    "    locations = locations.astype(\"category\")\n",
    "    clobbered = clobbered.merge(locations, how=\"left\", on=\"stationid\")\n",
    "\n",
    "    # export to file\n",
    "    print(\"exporting master stations\")\n",
    "    exportpath = \"data/stations_master.parquet\"\n",
    "    clobbered.to_parquet(exportpath)\n",
    "    print(\"master stations saved to parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...data/stations_2019.parquet\n",
      "formatting...data/stations_2019.parquet\n",
      "concating...data/stations_2019.parquet\n",
      "unloaded...data/stations_2019.parquet\n",
      "loading...data/stations_2018.parquet\n",
      "formatting...data/stations_2018.parquet\n",
      "concating...data/stations_2018.parquet\n",
      "unloaded...data/stations_2018.parquet\n",
      "formatting master file\n",
      "formatting complete\n",
      "loading external station details\n",
      "extracting capacity\n",
      "reverse geocoding boro and neighbourhood, wait 15-20 mins...\n",
      "geocode complete, merging...\n",
      "exporting master stations\n",
      "master stations saved to parquet\n"
     ]
    }
   ],
   "source": [
    "years = [2019, 2018]  # , 2017, 2016, 2015]\n",
    "clobber_master_stations(years)\n",
    "\n",
    "# there are some missing capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_parquet(\"data/stations_master.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationid</th>\n",
       "      <th>stationname</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>capacity</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3160.0</td>\n",
       "      <td>Central Park West &amp; W 76 St</td>\n",
       "      <td>40.778968</td>\n",
       "      <td>-73.973747</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10023-5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519.0</td>\n",
       "      <td>Pershing Square North</td>\n",
       "      <td>40.751873</td>\n",
       "      <td>-73.977706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murray Hill</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3171.0</td>\n",
       "      <td>Amsterdam Ave &amp; W 82 St</td>\n",
       "      <td>40.785247</td>\n",
       "      <td>-73.976673</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Manhattan Community Board 7</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504.0</td>\n",
       "      <td>1 Ave &amp; E 16 St</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Manhattan Community Board 6</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229.0</td>\n",
       "      <td>Great Jones St</td>\n",
       "      <td>40.727434</td>\n",
       "      <td>-73.993790</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NoHo</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>10012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stationid                  stationname   latitude  longitude  capacity  \\\n",
       "0     3160.0  Central Park West & W 76 St  40.778968 -73.973747      39.0   \n",
       "1      519.0        Pershing Square North  40.751873 -73.977706       NaN   \n",
       "2     3171.0      Amsterdam Ave & W 82 St  40.785247 -73.976673      39.0   \n",
       "3      504.0              1 Ave & E 16 St  40.732219 -73.981656      54.0   \n",
       "4      229.0               Great Jones St  40.727434 -73.993790      23.0   \n",
       "\n",
       "                 neighbourhood     suburb    postcode  \n",
       "0                          NaN  Manhattan  10023-5104  \n",
       "1                  Murray Hill  Manhattan       10037  \n",
       "2  Manhattan Community Board 7  Manhattan       10024  \n",
       "3  Manhattan Community Board 6  Manhattan       10009  \n",
       "4                         NoHo  Manhattan       10012  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Rebalances Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bad code  #intentially break if run all cells is performed\n",
    "\n",
    "# clobber all old nyc CSVs NOTE THIS CRASHES COMPUTER\n",
    "\n",
    "\n",
    "# nyc_old_dfs = []\n",
    "# for file in nyc_old:\n",
    "#     print(f'file {NY_DIR + file}')\n",
    "#     df = pd.read_csv(NY_DIR + file)\n",
    "#     nyc_old_dfs.append(df)\n",
    "#\n",
    "# nyc_old_df = pd.concat(nyc_old_dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# ddf = dd.read_csv(nyc_old,\n",
    "#                   dtype={'birth year': 'object',\n",
    "#                          'end station id': 'float64'})\n",
    "#\n",
    "# # columns are Sentence Cased for some CSVs and lower cased for others\n",
    "# ddf = ddf.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ddf.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO only works for old schema at the moment\n",
    "def summarise_months(outfilename: str, months: list):\n",
    "    \"\"\"\n",
    "    Writes monthly summary given list of monthly trip data\n",
    "\n",
    "    :param outfilename: where to write the summary csv\n",
    "    :param months: list of CSVs for the monthly trip data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for file in months:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        # logging.debug(f'{file}: {list(df.columns)}')\n",
    "\n",
    "        year_month = file.split(\"/\")[-1].removesuffix(\".csv\")  # YYYYMM\n",
    "\n",
    "        summary = pd.Series(dtype=object)\n",
    "        summary[\"datetime\"] = year_month\n",
    "        summary[\"counttrips\"] = df.shape[0]\n",
    "        summary[\"meanduration\"] = df.tripduration.mean()\n",
    "        summary[\"modestartstationid\"] = df.startstationid.mode()\n",
    "        summary[\"modestartstationname\"] = df.startstationname.mode()\n",
    "        summary[\"modestartstationlatitude\"] = df.startstationlatitude.mode()\n",
    "        summary[\"modestartstationlongitude\"] = df.startstationlongitude.mode()\n",
    "        summary[\"modeendstationid\"] = df.endstationid.mode()\n",
    "        summary[\"modeendstationname\"] = df.endstationname.mode()\n",
    "        summary[\"modeendstationlatitude\"] = df.endstationlatitude.mode()\n",
    "        summary[\"modeendstationlongitude\"] = df.endstationlongitude.mode()\n",
    "\n",
    "        if \"usertype\" in df.columns:\n",
    "            summary[\"usertypevalues\"] = df.usertype.value_counts()\n",
    "        elif \"member_casual\" in df.columns:\n",
    "            summary[\"usertypevalues\"] = df.member_casual.value_counts()\n",
    "\n",
    "        if \"gender\" in df.columns:\n",
    "            summary[\"gendervalues\"] = df.gender.value_counts()\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    summary_df = pd.DataFrame()\n",
    "    summary_df = summary_df.append(\n",
    "        summaries\n",
    "    )  # TODO use concat instead to suppress warning\n",
    "    summary_df.set_index(\"datetime\")\n",
    "    summary_df.to_csv(outfilename)\n",
    "\n",
    "\n",
    "# write summary data month by month for NYC and NJ\n",
    "summarise_months(DATA_DIR + \"summary_nyc_old_schema.csv\", nyc_old)\n",
    "summarise_months(DATA_DIR + \"summary_jc_old_schema.csv\", jc_old)\n",
    "\n",
    "# read summary\n",
    "nyc_old_schema_summary = pd.read_csv(\"data/summary_nyc_old_schema.csv\", index_col=0)\n",
    "nyc_old_schema_summary\n",
    "\n",
    "# read JC summary\n",
    "jc_old_schema_summary = pd.read_csv(\"data/summary_nyc_old_schema.csv\", index_col=0)\n",
    "jc_old_schema_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original-ish Clober with logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clobber_year(year=2019, state=\"NY\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe from source CSVs that is all monthly trip data for that `year`\n",
    "\n",
    "    :param year: the year for which to concatenate data files\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: the merged dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    range_start = str(year) + \"-01\"\n",
    "    range_end = str(year) + \"-13\"  # Not sure why I have to select 13 here...\n",
    "    files = None\n",
    "    if state == \"NY\":\n",
    "        files = sorted(\n",
    "            [NY_DIR + f for f in os.listdir(NY_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    elif state == \"NJ\":\n",
    "        files = sorted(\n",
    "            [NJ_DIR + f for f in os.listdir(NJ_DIR) if range_start <= f <= range_end]\n",
    "        )\n",
    "    else:\n",
    "        raise IndexError(f\"No data for state: {state}\")\n",
    "\n",
    "    logging.debug(f\"Will merge these files: {files}, number of files: {len(files)}\")\n",
    "\n",
    "    # Concatenate all monthly data in range\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.lower().replace(\" \", \"\") for col in df.columns]\n",
    "        logging.debug(f\"Appending df file: {file}...\")\n",
    "        dfs.append(df)\n",
    "        del df\n",
    "        gc.collect()\n",
    "    logging.debug(f\"Merging dataframes...\")\n",
    "    clobbered = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # unload temp variables\n",
    "    del dfs\n",
    "    gc.collect()\n",
    "\n",
    "    # update dtypes (doesn't carry through concat if done on read_csv...?)\n",
    "    coltypes = {\n",
    "        \"tripduration\": \"int32\",\n",
    "        \"starttime\": \"datetime64\",\n",
    "        \"stoptime\": \"datetime64\",\n",
    "        \"startstationid\": \"category\",\n",
    "        \"startstationname\": \"category\",\n",
    "        \"startstationlatitude\": \"category\",\n",
    "        \"startstationlongitude\": \"category\",\n",
    "        \"endstationid\": \"category\",\n",
    "        \"endstationname\": \"category\",\n",
    "        \"endstationlatitude\": \"category\",\n",
    "        \"endstationlongitude\": \"category\",\n",
    "        \"bikeid\": \"category\",\n",
    "        \"usertype\": \"category\",\n",
    "        \"birthyear\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "    }\n",
    "    clobbered = clobbered.astype(coltypes)\n",
    "    print(year, \"...dtypes converted\")\n",
    "\n",
    "    return clobbered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_files(\n",
    "    years=[2019], state=\"NY\"\n",
    ") -> pd.DataFrame:  # what does -> pd.dataframe do?\n",
    "    \"\"\"\n",
    "    Calls clobber_year and writes output to both csv and parquet to `data/`\n",
    "\n",
    "    :param years: list of years to generate data files for\n",
    "    :param state: 'NY' or 'NJ'. default 'NY'\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "    for year in years:\n",
    "        # clobber dataframe\n",
    "        print(\"clobbering...\", year)\n",
    "        temp_df = clobber_year(year, state)\n",
    "        print(year, \"...clobbered\")\n",
    "\n",
    "        # extract station data [only if uberparquet faisl]\n",
    "        # perform reverse geocode ---> later\n",
    "        # create yearly stations dataframe\n",
    "        # save to file\n",
    "        # unload related dfs\n",
    "\n",
    "        # clean dataframe\n",
    "        # drop NAs? - not implemented atm\n",
    "        # drop station cols\n",
    "\n",
    "        # save to files\n",
    "        exportpath = \"data/rides_\" + str(year) + \".parquet\"\n",
    "        temp_df.to_parquet(exportpath)\n",
    "        print(year, \"...saved to parquet\")\n",
    "\n",
    "        exportpath = \"data/rides_\" + str(year) + \".csv\"\n",
    "        temp_df.to_csv(exportpath)\n",
    "        print(year, \"...saved to csv\")\n",
    "\n",
    "        # unload dataframe\n",
    "        del temp_df\n",
    "        gc.collect()\n",
    "        print(year, \"...unloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
