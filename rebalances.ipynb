{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rebalances EDA\n",
    "Citibike does not provide data regarding bike rebalances, however, a bike that starts from a station where it did not end its previous trip it likely was either rebalanced or taken out of service. We will assume the former is the case for this preliminary exercise and consider ways to make this more robust in the future.\n",
    "\n",
    "This Notebook creates rebalance tables for years 2014-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Annual Rebalance Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting... 2013 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2014 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2015 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2016 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2017 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2018 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2019 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2020 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "starting... 2021 ----------------------\n",
      "loading...\n",
      "sorting...\n",
      "creating trips1...\n",
      "creating trips2...\n",
      "concating trips1 and trips 2...\n",
      "filtering....\n",
      "formatting...\n",
      "exporting to parquet...\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# set import columns and preferred dtypes\n",
    "trip_columns = [\n",
    "    \"starttime\",\n",
    "    \"stoptime\",\n",
    "    \"startstationid\",\n",
    "    \"endstationid\",\n",
    "    \"bikeid\",\n",
    "]\n",
    "col_types = {\n",
    "    \"starttime\": \"datetime64\",\n",
    "    \"stoptime\": \"datetime64\",\n",
    "    \"startstationid\": \"category\",\n",
    "    \"endstationid\": \"category\",\n",
    "    \"bikeid\": \"category\",\n",
    "}\n",
    "\n",
    "# create an dummy dataframe to offset when merging\n",
    "offset = pd.DataFrame(\n",
    "    {\n",
    "        \"starttime\": pd.to_datetime(\"2010-09-01\"),\n",
    "        \"startstationid\": 0,\n",
    "        \"stoptime\": pd.to_datetime(\"2010-09-01\"),\n",
    "        \"endstationid\": 0,\n",
    "        \"bikeid\": 0,\n",
    "    },\n",
    "    index=[0],\n",
    ")\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # load data from parquet\n",
    "    print(\"starting...\", year, \"----------------------\")\n",
    "    print(\"loading...\")\n",
    "    trips = pd.read_parquet(\n",
    "        \"data/tripdata_parquet/NY/\" + str(year) + \".parquet\",\n",
    "        engine=\"pyarrow\",\n",
    "        columns=trip_columns,\n",
    "    ).reset_index()\n",
    "    trips.drop(trips.columns[0], axis=1, inplace=True)  # drop the dask index\n",
    "\n",
    "    # order trips sequentially by bike\n",
    "    print(\"sorting...\")\n",
    "    trips = trips.sort_values(by=[\"bikeid\", \"starttime\"])\n",
    "\n",
    "    # offset rides1 (start stations) to track end station, rides 2 for start station\n",
    "    print(\"creating trips1...\")\n",
    "    trips1 = (\n",
    "        pd.concat([offset, trips])\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"bikeid\": \"bikeid1\"})\n",
    "    )\n",
    "\n",
    "    print(\"creating trips2...\")\n",
    "    trips2 = (\n",
    "        pd.concat([trips, offset])\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"bikeid\": \"bikeid2\"})\n",
    "    )\n",
    "\n",
    "    # concat horizontally - a ride would start from the same endstation unless rebalanced\n",
    "    print(\"concating trips1 and trips 2...\")\n",
    "    trips = pd.concat(\n",
    "        [\n",
    "            trips1[[\"bikeid1\", \"stoptime\", \"endstationid\"]],\n",
    "            trips2[[\"bikeid2\", \"starttime\", \"startstationid\"]],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove temp dataframes from memory\n",
    "    del [trips1, trips2]\n",
    "    gc.collect()\n",
    "\n",
    "    # filter using rebalance criteria\n",
    "    print(\"filtering....\")\n",
    "    trips = trips.loc[\n",
    "        (trips.bikeid1 == trips.bikeid2) & (trips.startstationid != trips.endstationid)\n",
    "    ]\n",
    "    trips.drop(columns=[\"bikeid2\"], inplace=True)\n",
    "\n",
    "    # format and export\n",
    "    print(\"formatting...\")\n",
    "    trips.rename(\n",
    "        columns={\n",
    "            \"bikeid1\": \"bikeid\",\n",
    "            \"stoptime\": \"prevtrip_stoptime\",\n",
    "            \"endstationid\": \"prevtrip_endstationid\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    print(\"exporting to parquet...\")\n",
    "    rebal_filepath = \"data/rebalance_parquet/\" + str(year) + \"_rebalances.parquet\"\n",
    "    trips.to_parquet(rebal_filepath, engine=\"pyarrow\")\n",
    "\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Rebalance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending...data/rebalance_parquet/2014_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2015_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2016_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2017_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2018_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2019_rebalances.parquet\n",
      "appending...data/rebalance_parquet/2020_rebalances.parquet\n",
      "concating dfs...\n",
      "exporting...\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "years = [\n",
    "    2014,\n",
    "    2015,\n",
    "    2016,\n",
    "    2017,\n",
    "    2018,\n",
    "    2019,\n",
    "    2020,\n",
    "]  # 2013 and 2021+ intientionally excluded\n",
    "\n",
    "rebal_dfs = []\n",
    "rebal_files = [\n",
    "    \"data/rebalance_parquet/\" + str(y) + \"_rebalances.parquet\" for y in years\n",
    "]\n",
    "\n",
    "# create list of dataframes and concat\n",
    "for rebal_file in rebal_files:\n",
    "    print(\"appending...\" + rebal_file)\n",
    "    rebal_dfs.append(pd.read_parquet(rebal_file))\n",
    "print(\"concating dfs...\")\n",
    "rebal = pd.concat(rebal_dfs)\n",
    "\n",
    "# export to parquet\n",
    "print(\"exporting...\")\n",
    "rebal.to_parquet(\"data/rebalance_parquet/all_rebalances.parquet\", engine=\"pyarrow\")\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Featured Rebalance Table\n",
    "Merge w/ stations data to include features like boro and elevation for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "stations = pd.read_parquet(\"data/stations/stations.parquet\")\n",
    "rebal = pd.read_parquet(\"data/rebalance_parquet/all_rebalances.parquet\")\n",
    "\n",
    "# merge startstation features\n",
    "col_rename = {\n",
    "    \"stationid\": \"startstationid\",\n",
    "    \"stationname\": \"startstationname\",\n",
    "    \"capacity\": \"startcapacity\",\n",
    "    \"neighbourhood\": \"startneighborhood\",\n",
    "    \"boro\": \"startboro\",\n",
    "    \"elevation_ft\": \"startelevation_ft\",\n",
    "}\n",
    "stations.rename(columns=col_rename, inplace=True)\n",
    "stations.drop(columns=([\"latitude\", \"longitude\", \"zipcode\"]), inplace=True)\n",
    "rebal = rebal.merge(stations, how=\"left\", on=\"startstationid\")\n",
    "\n",
    "# merge prevtrip_endstation features\n",
    "col_rename = {\n",
    "    \"stationid\": \"prevtrip_endstationid\",\n",
    "    \"stationname\": \"prevtrip_endstationname\",\n",
    "    \"capacity\": \"prevtrip_capacity\",\n",
    "    \"neighbourhood\": \"prevtrip_neighborhood\",\n",
    "    \"boro\": \"prevtrip_boro\",\n",
    "    \"elevation_ft\": \"prevtrip_elevation_ft\",\n",
    "}\n",
    "stations = pd.read_parquet(\"data/stations/stations.parquet\")\n",
    "stations.rename(columns=col_rename, inplace=True)\n",
    "stations.drop(columns=([\"latitude\", \"longitude\", \"zipcode\"]), inplace=True)\n",
    "rebal = rebal.merge(stations, how=\"left\", on=\"prevtrip_endstationid\")\n",
    "\n",
    "rebal.to_parquet(\n",
    "    \"data/rebalance_parquet/all_rebalances_features.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df166cf616f39a8b0c2f66e83ba2afdd6bfc8b57008690bcf278f179dbfb1b31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
