{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rebalances Table Creation\n",
    "Citibike does not provide data regarding bike rebalances, however, a bike that starts from a station where it did not end its previous trip it likely was either rebalanced or taken out of service. We will assume the former is the case for this preliminary exercise and consider ways to make this more robust in the future.\n",
    "\n",
    "This Notebook creates rebalance tables for years 2014-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "REBALANCE_DIR = \"data/rebalance_parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(REBALANCE_DIR):\n",
    "    os.makedirs(os.path.dirname(REBALANCE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Annual Rebalance Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2020, 2021]  # [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# set import columns and preferred dtypes\n",
    "trip_columns = [\n",
    "    \"starttime\",\n",
    "    \"stoptime\",\n",
    "    \"startstationid\",\n",
    "    \"endstationid\",\n",
    "    \"bikeid\",\n",
    "]\n",
    "col_types = {\n",
    "    \"starttime\": \"datetime64\",\n",
    "    \"stoptime\": \"datetime64\",\n",
    "}\n",
    "\n",
    "# create an dummy dataframe to offset when merging\n",
    "offset = pd.DataFrame(\n",
    "    {\n",
    "        \"starttime\": pd.to_datetime(\"2010-09-01\"),\n",
    "        \"startstationid\": 0,\n",
    "        \"stoptime\": pd.to_datetime(\"2010-09-01\"),\n",
    "        \"endstationid\": 0,\n",
    "        \"bikeid\": 0,\n",
    "    },\n",
    "    index=[0],\n",
    ")\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # load data from parquet\n",
    "    print(\"starting...\", year, \"----------------------\")\n",
    "    print(\"loading...\")\n",
    "    trips = pd.read_parquet(\n",
    "        \"data/tripdata_parquet/NY/\" + str(year) + \".parquet\",\n",
    "        engine=\"pyarrow\",\n",
    "        columns=trip_columns,\n",
    "    ).reset_index()\n",
    "    trips.drop(trips.columns[0], axis=1, inplace=True)  # drop the dask index\n",
    "\n",
    "    # convert date cols to enable sorting\n",
    "    print(\"converting date types...\")\n",
    "    trips = trips.astype(col_types)\n",
    "\n",
    "    # order trips sequentially by bike and start time\n",
    "    print(\"sorting...\")\n",
    "    trips = trips.sort_values(by=[\"bikeid\", \"starttime\"])\n",
    "\n",
    "    # offset rides1 (start stations) to track end station, rides 2 for start station\n",
    "    print(\"creating trips1...\")\n",
    "    trips1 = (\n",
    "        pd.concat([offset, trips])\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"bikeid\": \"bikeid1\"})\n",
    "    )\n",
    "\n",
    "    print(\"creating trips2...\")\n",
    "    trips2 = (\n",
    "        pd.concat([trips, offset])\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"bikeid\": \"bikeid2\"})\n",
    "    )\n",
    "\n",
    "    # concat horizontally - a ride would start from the same endstation unless rebalanced\n",
    "    print(\"concating trips1 and trips 2...\")\n",
    "    rebal = pd.concat(\n",
    "        [\n",
    "            trips1[[\"bikeid1\", \"stoptime\", \"endstationid\"]],\n",
    "            trips2[[\"bikeid2\", \"starttime\", \"startstationid\"]],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove temp dataframes from memory\n",
    "    del [trips1, trips2]\n",
    "    gc.collect()\n",
    "\n",
    "    # filter using rebalance criteria\n",
    "    print(\"filtering....\")\n",
    "    rebal = rebal.loc[\n",
    "        (rebal.bikeid1 == rebal.bikeid2) & (rebal.startstationid != rebal.endstationid)\n",
    "    ]\n",
    "    rebal.drop(columns=[\"bikeid2\"], inplace=True)\n",
    "\n",
    "    # format\n",
    "    print(\"formatting...\")\n",
    "    rebal.rename(\n",
    "        columns={\n",
    "            \"bikeid1\": \"bikeid\",\n",
    "            \"stoptime\": \"prevtrip_stoptime\",\n",
    "            \"endstationid\": \"prevtrip_endstationid\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # add rebalance time row\n",
    "    rebal[\"rebal_time_hr\"] = (\n",
    "        rebal.starttime - rebal.prevtrip_stoptime\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    # catch bad trip records (indicates a ride was started while anothe ride with that bikeid was in progress)\n",
    "    neg = rebal.loc[rebal.rebal_time_hr < 0]\n",
    "\n",
    "    # removes first offending record from rides table and continues looping until none are left\n",
    "    # loop is required because fixing first real bad record *may or may not* fix subsequent bad records for a given bike\n",
    "    # see section at end of rebalance_eda notebook for more details on this issue\n",
    "    if neg.shape[0] > 0:\n",
    "        print(\"up to\", neg.shape[0], \"bad records found - begin drop loop...\")\n",
    "        count = 0\n",
    "\n",
    "    while neg.shape[0] > 0:\n",
    "        count += 1\n",
    "        print(\"dropping bad record\", count)\n",
    "        drop_trip = neg.prevtrip_stoptime.iloc[0]\n",
    "        trips = trips[trips.stoptime != drop_trip]\n",
    "\n",
    "        # offset rides1 (start stations) to track end station, rides 2 for start station\n",
    "        trips1 = (\n",
    "            pd.concat([offset, trips])\n",
    "            .reset_index(drop=True)\n",
    "            .rename(columns={\"bikeid\": \"bikeid1\"})\n",
    "        )\n",
    "\n",
    "        trips2 = (\n",
    "            pd.concat([trips, offset])\n",
    "            .reset_index(drop=True)\n",
    "            .rename(columns={\"bikeid\": \"bikeid2\"})\n",
    "        )\n",
    "\n",
    "        # concat horizontally - a ride would start from the same endstation unless rebalanced\n",
    "        rebal = pd.concat(\n",
    "            [\n",
    "                trips1[[\"bikeid1\", \"stoptime\", \"endstationid\"]],\n",
    "                trips2[[\"bikeid2\", \"starttime\", \"startstationid\"]],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # remove temp dataframes from memory\n",
    "        del [trips1, trips2]\n",
    "        gc.collect()\n",
    "\n",
    "        # filter using rebalance criteria\n",
    "        rebal = rebal.loc[\n",
    "            (rebal.bikeid1 == rebal.bikeid2)\n",
    "            & (rebal.startstationid != rebal.endstationid)\n",
    "        ]\n",
    "        rebal.drop(columns=[\"bikeid2\"], inplace=True)\n",
    "\n",
    "        # format and export\n",
    "        rebal.rename(\n",
    "            columns={\n",
    "                \"bikeid1\": \"bikeid\",\n",
    "                \"stoptime\": \"prevtrip_stoptime\",\n",
    "                \"endstationid\": \"prevtrip_endstationid\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # add rebalance time row\n",
    "        rebal[\"rebal_time_hr\"] = (\n",
    "            rebal.starttime - rebal.prevtrip_stoptime\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "        # update negative rebalances\n",
    "        neg = rebal.loc[rebal.rebal_time_hr < 0]\n",
    "\n",
    "    # export\n",
    "    print(\"exporting to parquet...\")\n",
    "    rebal_filepath = \"data/rebalance_parquet/\" + str(year) + \"_rebalances.parquet\"\n",
    "    rebal.to_parquet(rebal_filepath, engine=\"pyarrow\")\n",
    "\n",
    "    # remove df from memory before loop\n",
    "    del trips\n",
    "    del rebal\n",
    "    gc.collect()\n",
    "\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Rebalance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\n",
    "    2014,\n",
    "    2015,\n",
    "    2016,\n",
    "    2017,\n",
    "    2018,\n",
    "    2019,\n",
    "    2020,\n",
    "]  # 2013 and 2021+ intentionally excluded, 2021 potentially used as test dataset for rebalance model (if we get to it)\n",
    "\n",
    "rebal_dfs = []\n",
    "rebal_files = [\n",
    "    \"data/rebalance_parquet/\" + str(y) + \"_rebalances.parquet\" for y in years\n",
    "]\n",
    "\n",
    "# create list of dataframes and concat\n",
    "for rebal_file in rebal_files:\n",
    "    print(\"appending...\" + rebal_file)\n",
    "    rebal_dfs.append(pd.read_parquet(rebal_file))\n",
    "print(\"concating dfs...\")\n",
    "rebal = pd.concat(rebal_dfs)\n",
    "\n",
    "# export to parquet\n",
    "print(\"exporting...\")\n",
    "rebal.to_parquet(\"data/rebalance_parquet/all_rebalances.parquet\", engine=\"pyarrow\")\n",
    "del rebal\n",
    "gc.collect()\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Featured Rebalance Table\n",
    "Merge w/ stations data to include features like boro and elevation for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "stations = pd.read_parquet(\"data/stations/stations.parquet\")\n",
    "rebal = pd.read_parquet(\"data/rebalance_parquet/all_rebalances.parquet\")\n",
    "\n",
    "# merge startstation features\n",
    "col_rename = {\n",
    "    \"stationid\": \"startstationid\",\n",
    "    \"stationname\": \"startstationname\",\n",
    "    \"capacity\": \"startcapacity\",\n",
    "    \"neighbourhood\": \"startneighborhood\",\n",
    "    \"boro\": \"startboro\",\n",
    "    \"elevation_ft\": \"startelevation_ft\",\n",
    "}\n",
    "stations.rename(columns=col_rename, inplace=True)\n",
    "stations.drop(columns=([\"latitude\", \"longitude\", \"zipcode\"]), inplace=True)\n",
    "rebal = rebal.merge(stations, how=\"left\", on=\"startstationid\")\n",
    "\n",
    "# merge prevtrip_endstation features\n",
    "col_rename = {\n",
    "    \"stationid\": \"prevtrip_endstationid\",\n",
    "    \"stationname\": \"prevtrip_endstationname\",\n",
    "    \"capacity\": \"prevtrip_capacity\",\n",
    "    \"neighbourhood\": \"prevtrip_neighborhood\",\n",
    "    \"boro\": \"prevtrip_boro\",\n",
    "    \"elevation_ft\": \"prevtrip_elevation_ft\",\n",
    "}\n",
    "stations = pd.read_parquet(\"data/stations/stations.parquet\")\n",
    "stations.rename(columns=col_rename, inplace=True)\n",
    "stations.drop(columns=([\"latitude\", \"longitude\", \"zipcode\"]), inplace=True)\n",
    "rebal = rebal.merge(stations, how=\"left\", on=\"prevtrip_endstationid\")\n",
    "\n",
    "rebal.to_parquet(\n",
    "    \"data/rebalance_parquet/all_rebalances_features.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebal.startboro.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pairwise Rebalance Table (just for mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebal = pd.read_parquet(\"data/rebalance_parquet/all_rebalances_features.parquet\")\n",
    "rebal = rebal.astype({\"starttime\": \"datetime64\"})\n",
    "rebal[\"rebal_year\"] = pd.DatetimeIndex(rebal[\"starttime\"]).year\n",
    "rebal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by pairs and count number per pair\n",
    "rebpair = (\n",
    "    rebal.groupby([\"rebal_year\", \"startstationname\", \"prevtrip_endstationname\"])[\n",
    "        [\"bikeid\"]\n",
    "    ]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"bikeid\", ascending=False)\n",
    ")\n",
    "rebpair.rename(columns={\"bikeid\": \"rebal_count\"}, inplace=True)\n",
    "\n",
    "# filter for only pairs that have been rebalanced more than ~10 times\n",
    "rebpair = rebpair.loc[rebpair.rebal_count > 10]\n",
    "\n",
    "# add id columns\n",
    "rebpair = rebpair.merge(\n",
    "    rebal[[\"startstationid\", \"startstationname\"]].drop_duplicates(\n",
    "        subset=\"startstationname\"\n",
    "    ),\n",
    "    how=\"left\",\n",
    "    on=\"startstationname\",\n",
    ").merge(\n",
    "    rebal[[\"prevtrip_endstationid\", \"prevtrip_endstationname\"]].drop_duplicates(\n",
    "        subset=\"prevtrip_endstationname\"\n",
    "    ),\n",
    "    how=\"left\",\n",
    "    on=\"prevtrip_endstationname\",\n",
    ")\n",
    "rebpair.rename(\n",
    "    columns={\n",
    "        \"startstationid\": \"stationid_to\",\n",
    "        \"prevtrip_endstationid\": \"stationid_from\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# create a single string for rebalance route\n",
    "rebpair[\"rebal_route\"] = (\n",
    "    rebpair.prevtrip_endstationname.astype(\"str\")\n",
    "    + \" to \"\n",
    "    + rebpair.startstationname.astype(\"str\")\n",
    ")\n",
    "\n",
    "rebpair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebpair.to_parquet(\"data/rebalance_parquet/rebalance_pairs.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df166cf616f39a8b0c2f66e83ba2afdd6bfc8b57008690bcf278f179dbfb1b31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
